{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Ds-Bjv3uLdK",
    "outputId": "1214e356-d46d-4b9b-a640-ed4c03f349e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-22 03:08:12.385876: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "import datetime\n",
    "import time\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import glob\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8q7adwLNuMKl"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uB8Gfp23uTml"
   },
   "outputs": [],
   "source": [
    "# %cd /content/drive/My Drive/Colab Notebooks/Dissertation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "F6f-fGbAs68-"
   },
   "outputs": [],
   "source": [
    "# !pip install pyyaml h5py  # Required to save models in HDF5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "y3zDA204uLdO"
   },
   "outputs": [],
   "source": [
    "def get_house_path(house):\n",
    "    return f'data/ukdale-parsed-chunks/house_{house}/'\n",
    "    \n",
    "def get_chunk_path(house, chunk):\n",
    "    return get_house_path(house) + f'chunk_{chunk}.dat'\n",
    "\n",
    "def get_num_chunks(house):\n",
    "    return len(glob.glob(get_house_path(house) + 'chunk_*[0-9].dat'))\n",
    "\n",
    "def read_file(house, chunk, labels):\n",
    "    file = get_chunk_path(house, chunk)\n",
    "    print(f'reading file {file}; for house {house} and chunk {chunk}');\n",
    "    \n",
    "    dtypes = {}\n",
    "    for label in labels[house]:\n",
    "        dtypes[label] = 'float32'\n",
    "    df = pd.read_table(file, sep = '\\t', header=0, names = labels[house], \n",
    "                                       dtype = dtypes) \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NEMReQenMhM4"
   },
   "outputs": [],
   "source": [
    "def parse_data(df):\n",
    "    df['timestamp'] = df['unix_time'].astype(\"datetime64[s]\")\n",
    "    df.set_index(df['timestamp'].values, inplace=True)\n",
    "    df.drop(['unix_time'], axis=1, inplace=True)\n",
    "\n",
    "    df['timeslice'] = df.timestamp.dt.hour\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "tDUl5LXkMhM4"
   },
   "outputs": [],
   "source": [
    "def read_labels():\n",
    "    labels = {}\n",
    "    for house in range(1, 2):\n",
    "        fileName = get_chunk_path(house, 1)\n",
    "        house_labels = pd.read_csv(fileName, sep = '\\t', nrows=1).columns.tolist()\n",
    "        labels[house] = house_labels\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "bhFSehd2uLdQ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_house_data_generator(house):\n",
    "    labels = read_labels()\n",
    "    num_chunks = get_num_chunks(house)\n",
    "    for i in range(1, num_chunks + 1):\n",
    "        if int(i) == 1:\n",
    "            print(f'reading house {house}; chunk 1');\n",
    "\n",
    "        df = read_file(house, i, labels)\n",
    "        df = parse_data(df)\n",
    "        \n",
    "        print(f'read house {house}; chunk {i}; df.shape is {df.shape}')\n",
    "    \n",
    "        yield df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "iw61Z97LbYxm"
   },
   "outputs": [],
   "source": [
    "def get_merged_chunks(house, num_chunks):\n",
    "    max_num_chunks = get_num_chunks(house)\n",
    "    num_chunks = max_num_chunks if num_chunks > max_num_chunks else num_chunks\n",
    "    house_gen = get_house_data_generator(house)\n",
    "    data = [next(house_gen) for i in range(num_chunks)]\n",
    "\n",
    "    return pd.concat(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "S1LswQmQMhM6"
   },
   "outputs": [],
   "source": [
    "def get_all_data_generators():\n",
    "    gen = {}\n",
    "    for house in range(1,2):\n",
    "        gen[house] = get_house_data_generator(house)\n",
    "        \n",
    "    return gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "t9Rdc6aQMhM6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "House 1:  ['mains_active', 'mains_apparent', 'mains_rms', 'aggregate_apparent', 'ft_boiler', 'ft_solar_thermal_pump', 'ft_washing_machine', 'ft_dishwasher', 'ft_tv', 'ft_kitchen_lights', 'ft_htpc', 'ft_kettle', 'ft_toaster', 'ft_fridge', 'ft_microwave', 'ft_amp_livingroom', 'ft_adsl_router', 'ft_livingroom_s_lamp', 'ft_lighting_circuit', 'ft_subwoofer_livingroom', 'ft_livingroom_lamp_tv', 'ft_kitchen_phone&stereo', 'ft_coffee_machine', 'ft_gas_oven', 'ft_data_logger_pc', 'ft_office_lamp2', 'aggregate_active', 'unix_time'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels = read_labels()\n",
    "for house in range(1,2):\n",
    "    print('House {}: '.format(house), labels[house], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "QRKH9QNAMhM8"
   },
   "outputs": [],
   "source": [
    "# gen = get_all_data_generators()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Latc0E1GMhM8"
   },
   "outputs": [],
   "source": [
    "# house_gen = get_house_data_generator(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ewxyz61CuLdR"
   },
   "outputs": [],
   "source": [
    "def print_heads_tails(df, h=True,t=True):\n",
    "    print(f'House {house}, data has shape: {df.shape}')\n",
    "    if h:\n",
    "        display(df.head(2))\n",
    "\n",
    "    print()\n",
    "\n",
    "    if t:\n",
    "        display(df.tail(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NRv9JU8iMhM9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading house 1; chunk 1\n",
      "reading file data/ukdale-parsed-chunks/house_1/chunk_1.dat; for house 1 and chunk 1\n"
     ]
    }
   ],
   "source": [
    "ref_chunk_df = get_merged_chunks(1, 2)\n",
    "print_heads_tails(ref_chunk_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QToXzTqWuLdU"
   },
   "outputs": [],
   "source": [
    "def get_dates(house_df, house):\n",
    "    dates = [str(time)[:10] for time in house_df.index.values]\n",
    "    dates = sorted(list(set(dates)))\n",
    "    print('House {0} data contain {1} days from {2} to {3}.'.format(house,len(dates),dates[0], dates[-1]))\n",
    "    print(dates, '\\n')\n",
    "\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "64hC_1Y-MhM9"
   },
   "outputs": [],
   "source": [
    "dates = {}\n",
    "# for house in range(1,2):\n",
    "dates[1] = get_dates(ref_chunk_df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwNKZxCSjaOz"
   },
   "outputs": [],
   "source": [
    "# Use day 2, house 1, to get number of df rows per day\n",
    "rows_per_day = ref_chunk_df.loc[:dates[1][2]].shape[0]\n",
    "feature = 'ft_kettle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aCdlZrYSuLdV",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot first 10 days of device\n",
    "def plot_ft_days(df, label, n_days):\n",
    "    days_series = df.loc[:dates[house][n_days]][label]\n",
    "    print(days_series.shape)\n",
    "    days_series_bins = days_series.values.reshape(-1, n_days)\n",
    "    print(days_series_bins.shape)\n",
    "    fig, axes = plt.subplots((n_days+1)//2,2, figsize=(24, n_days*2) )\n",
    "    for i in range(days_series_bins.shape[-1]):\n",
    "        series = days_series_bins[:,i]\n",
    "        axes.flat[i].plot(series, alpha = 0.6)\n",
    "        axes.flat[i].set_title(f'day {i+1}', fontsize = '15')\n",
    "    plt.suptitle(f'First n_days for {label}', fontsize = '30')\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4MH-laOJMhM-"
   },
   "outputs": [],
   "source": [
    "plot_ft_days(ref_chunk_df, feature, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YlNeIup3uLdW"
   },
   "outputs": [],
   "source": [
    "# Separate house 1 data into train, validation and test data\n",
    "n_days = len(dates[house])\n",
    "train_index = math.ceil(n_days *0.5)\n",
    "test_index = math.ceil(n_days *0.8)\n",
    "df1_train = ref_chunk_df.loc[:dates[1][train_index]]\n",
    "df1_val = ref_chunk_df.loc[dates[1][train_index]:dates[1][test_index]]\n",
    "df1_test = ref_chunk_df.loc[dates[1][test_index]:]\n",
    "print('df_train.shape: ', df1_train.shape)\n",
    "print('df_val.shape: ', df1_val.shape)\n",
    "print('df_test.shape: ', df1_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8gFcUhDf2bHc"
   },
   "outputs": [],
   "source": [
    "# Global vars\n",
    "ref_max = df1_train[['mains_active']].values.max()\n",
    "seq_per_batch = 1000\n",
    "seq_len = 128\n",
    "# seq_per_batch = 64\n",
    "# seq_len = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iypb5JRrajIJ"
   },
   "outputs": [],
   "source": [
    "def reshape_x(x):\n",
    "    # return np.reshape(x, (x.shape[0], n_bins, 1))\n",
    "    return np.reshape(x, (x.shape[0], 1, 1))\n",
    "\n",
    "class KerasBatchGenerator(object):\n",
    "    # we default stride to 1 since we dont want future values to affect past values\n",
    "    def __init__(self, mains, meter, seq_per_batch, seq_len, stride=1):\n",
    "        assert(len(mains) == len(meter))\n",
    "        self.data_x = mains\n",
    "        self.data_y = meter\n",
    "        self.seq_per_batch = seq_per_batch # the number of seqeunces per batch\n",
    "        self.seq_len = seq_len # the length of each sequence\n",
    "        self.stride = stride\n",
    "        self.current_idx = 0\n",
    "        self.data_len = len(self.data_x)\n",
    "\n",
    "    def load_sequence(self, x, y, seq_index):\n",
    "        seq = self.data_x[self.current_idx:self.current_idx + self.seq_len]\n",
    "        # Pad initial sequences since they will surely have zero padding if seq_len > \n",
    "        x[seq_index, self.seq_len - len(seq):] = seq\n",
    "        y[seq_index] = self.data_y[self.current_idx + 1]\n",
    "\n",
    "    def generate(self):\n",
    "        x = np.zeros((self.seq_per_batch, self.seq_len, 1), dtype=np.float32)\n",
    "        y = np.zeros((self.seq_per_batch), dtype=np.float32)\n",
    "        while True:\n",
    "            for seq_index in range(self.seq_per_batch):\n",
    "                if self.current_idx + self.seq_len >= self.data_len:\n",
    "                    # reset the index back to the start of the data set\n",
    "                    self.current_idx = 0\n",
    "                self.load_sequence(x, y, seq_index)\n",
    "                self.current_idx += self.stride\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAeh6aGoktW5"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Dense, LSTM, Bidirectional, Flatten, Dropout, Reshape\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "def create_model():\n",
    "    # Rework of:\n",
    "    # https://github.com/JackKelly/neuralnilm_prototype/blob/2119292e7d5c8a137797ad3c9abf9f37e7f749af/scripts/e567.py\n",
    "    # https://github.com/OdysseasKr/neural-disaggregator/blob/master/DAE/daedisaggregator.py\n",
    "    model = Sequential()\n",
    "\n",
    "    # 1D Conv\n",
    "    model.add(Conv1D(8, 4, activation=\"linear\", padding=\"same\", strides=1))\n",
    "#     model.add(Conv1D(8, 4, activation=\"linear\", input_shape=(seq_len, 1), padding=\"same\", strides=1))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Fully Connected Layers\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense((seq_len-0)*8, activation='relu'))\n",
    "\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense((seq_len-0)*8, activation='relu'))\n",
    "\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # 1D Conv\n",
    "    model.add(Reshape(((seq_len-0), 8)))\n",
    "    model.add(Conv1D(1, 4, activation=\"linear\", padding=\"same\", strides=1))\n",
    "\n",
    "    adam_opt = Adam(lr = 1e-1)\n",
    "#     sgd_opt = SGD(lr = 1e-1, nesterov=True)\n",
    "    model.compile(loss='mse', optimizer=adam_opt,metrics=['accuracy', RootMeanSquaredError(), MeanAbsoluteError()])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3epob4fFuJxM"
   },
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WYcyfTObtx5r"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = f\"dae_training_{feature}/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "class TimingCallback(Callback):\n",
    "    def __init__(self, logs={}):\n",
    "        self.logs=[]\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.starttime = timer()\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.logs.append(timer()-self.starttime)\n",
    "\n",
    "def train(model, feature, epochs = 1):\n",
    "    x_train = df1_train[['mains_active']].values\n",
    "    x_val = df1_val[['mains_active']].values\n",
    "\n",
    "    y_train = df1_train[[feature]].values\n",
    "    y_val = df1_val[[feature]].values\n",
    "\n",
    "    y_train_norm = y_train / ref_max\n",
    "    y_val_norm = y_val / ref_max\n",
    "    x_train_norm = x_train / ref_max\n",
    "    x_val_norm = x_val / ref_max\n",
    "\n",
    "    print(x_train_norm.shape, y_train_norm.shape, x_val_norm.shape, y_val_norm.shape)\n",
    "\n",
    "    extra_train = seq_len - (len(x_train) % seq_len)\n",
    "\n",
    "    x_train_norm = np.append(x_train_norm, np.zeros(extra_train)).reshape((-1, 1))\n",
    "    y_train_norm = np.append(y_train_norm, np.zeros(extra_train)).reshape((-1, 1))\n",
    "\n",
    "    extra_val = seq_len - (len(x_val) % seq_len)\n",
    "    x_val_norm = np.append(x_val_norm,  np.zeros(extra_val)).reshape((-1, 1))\n",
    "    y_val_norm = np.append(y_val_norm,  np.zeros(extra_val)).reshape((-1, 1))\n",
    "    \n",
    "    print(x_train_norm.shape, y_train_norm.shape, x_val_norm.shape, y_val_norm.shape)\n",
    "\n",
    "    x_train_reshaped = np.reshape(x_train_norm, (len(x_train_norm) // seq_len, seq_len, 1))\n",
    "    y_train_reshaped = np.reshape(y_train_norm, (len(y_train_norm) // seq_len, seq_len, 1))\n",
    "    x_val_reshaped = np.reshape(x_val_norm, (len(x_val_norm) // seq_len, seq_len, 1))\n",
    "    y_val_reshaped = np.reshape(y_val_norm, (len(y_val_norm) // seq_len, seq_len, 1))\n",
    "\n",
    "    print(f'sequence_len {seq_len}')\n",
    "    print(f'epochs {epochs}')\n",
    "    print(f'Input initial shape {x_train.shape}; input final shape {x_train_reshaped.shape}')\n",
    "    print(f'Output initial shape {y_train.shape}; Output final shape {y_train_reshaped.shape}')\n",
    "\n",
    "    time_cb = TimingCallback()\n",
    "    cp_cb = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 save_freq=500,\n",
    "                                                 verbose=2)\n",
    "    callbacks = [time_cb, cp_cb]\n",
    "    \n",
    "        \n",
    "    n_train_batches = len(x_train_norm) // seq_len\n",
    "    n_val_batches = len(x_val_norm) // seq_len\n",
    "    train_data_generator = KerasBatchGenerator(x_train_norm, y_train_norm, seq_per_batch=n_train_batches, seq_len=seq_len)\n",
    "    val_data_generator = KerasBatchGenerator(x_val_norm, y_val_norm, seq_per_batch=n_val_batches, seq_len=seq_len)\n",
    "    \n",
    "#     history = model.fit(train_data_generator.generate(), steps_per_epoch=n_train_batches,\n",
    "#                     epochs=epochs,\n",
    "#                     validation_data=val_data_generator.generate(), callbacks=callbacks, workers=2)\n",
    "\n",
    "    history = model.fit(x_train_reshaped, y_train_reshaped, epochs=epochs, batch_size=seq_per_batch, validation_data=(x_val_reshaped, y_val_reshaped), callbacks=callbacks, verbose=2)\n",
    "    # history = model.fit(x_train_reshaped, y_train_reshaped, epochs=epochs, batch_size=seq_per_batch, validation_data=(x_val_reshaped, y_val_reshaped), callbacks=callbacks, shuffle=True)\n",
    "\n",
    "    return model, history, sum(time_cb.logs)\n",
    "\n",
    "def view_model_results(model, history):\n",
    "    fig = plt.figure()\n",
    "    fig.add_subplot(1,2,1)\n",
    "    plt.plot(history.history['loss'], label='train loss')\n",
    "    plt.plot(history.history['val_loss'], label='test loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('epoch')\n",
    "\n",
    "    # fig.add_subplot(1,2,2)\n",
    "    # plt.plot(history.history['accuracy'], label='train accuracy')\n",
    "    # plt.plot(history.history['val_accuracy'], label='test accuracy')\n",
    "    # plt.legend()\n",
    "    # plt.grid(True)\n",
    "    # plt.xlim([0,3])\n",
    "    # plt.ylim([0,1.0])\n",
    "    # plt.xlabel('epoch')\n",
    "\n",
    "    print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9mlxEOhJxS8p"
   },
   "outputs": [],
   "source": [
    "# Load model\n",
    "LOAD_MODEL=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ruu6OEQRuh4c"
   },
   "outputs": [],
   "source": [
    "if LOAD_MODEL:\n",
    "  # Loads the weights\n",
    "  model.load_weights(checkpoint_path)\n",
    "else:\n",
    "  model, history, time_spent = train(model, feature, epochs = 1000)\n",
    "  print('Time spent', time_spent)\n",
    "  view_model_results(model, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wf6Mrk0IaU7R"
   },
   "outputs": [],
   "source": [
    "def predict(model, x_test):\n",
    "    pred = model.predict(x_test)\n",
    "    \n",
    "    return pred\n",
    "\n",
    "def plot_prediction_windows(label, y_test, y_pred, use_active = False, n_samples = seq_len):\n",
    "    y_test_mean = y_test.mean()\n",
    "    n_plots_max = 16\n",
    "    n_rows = n_samples\n",
    "    n_plots = len(y_test) // n_rows\n",
    "    y_test_bins = np.resize(y_test, (n_rows, n_plots))\n",
    "    y_pred_bins = np.resize(y_pred, (n_rows, n_plots))\n",
    "    fig, axes = plt.subplots((n_plots_max+1)//2,2, figsize=(24, n_plots_max*2) )\n",
    "    plot_counter = 0\n",
    "    for i in range(y_test_bins.shape[-1]):\n",
    "        if plot_counter < n_plots_max:\n",
    "            y_test_series = y_test_bins[:,i]\n",
    "            y_pred_series = y_pred_bins[:,i]\n",
    "            if use_active: \n",
    "                window_mean = y_test_series.mean()\n",
    "                if y_test_series.mean() >= y_test_mean * 0.5:\n",
    "                    axes.flat[plot_counter].plot(y_test_series, color = 'blue', alpha = 0.6, label = 'True value')\n",
    "                    axes.flat[plot_counter].plot(y_pred_series, color = 'red', alpha = 0.6, label = 'Predicted value')\n",
    "                    axes.flat[plot_counter].set_title(f'window {plot_counter+1}; mean {window_mean}', fontsize = '15')\n",
    "                    plot_counter += 1\n",
    "            else: \n",
    "                axes.flat[plot_counter].plot(y_test_series, color = 'blue', alpha = 0.6, label = 'True value')\n",
    "                axes.flat[plot_counter].plot(y_pred_series, color = 'red', alpha = 0.6, label = 'Predicted value')\n",
    "                axes.flat[plot_counter].set_title(f'window {plot_counter+1}', fontsize = '15')\n",
    "                plot_counter += 1\n",
    "    plt.suptitle(f'Sample n_window predictions for {label}; data mean {y_test_mean} ', fontsize = '30')\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dqW2IQ0xaU7R"
   },
   "outputs": [],
   "source": [
    "x_test = df1_test[['mains_active']].values\n",
    "y_test = df1_test[[feature]].values\n",
    "original_len = len(x_test)\n",
    "\n",
    "x_test_norm = x_test / ref_max\n",
    "\n",
    "print(x_test.shape)\n",
    "\n",
    "extra_test = seq_len - (len(x_test) % seq_len)\n",
    "\n",
    "x_test_norm = np.append(x_test_norm, np.zeros(extra_test))\n",
    "x_test_reshaped = np.reshape(x_test_norm, (len(x_test_norm) // seq_len, seq_len, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M3hXevtCaU7S"
   },
   "outputs": [],
   "source": [
    "# Predict from test split\n",
    "y_pred = predict(model, x_test_reshaped)\n",
    "y_pred_reshaped = np.reshape(y_pred, (original_len + extra_test))[:original_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YCF5XuA1BnS4"
   },
   "outputs": [],
   "source": [
    "# Plot predictions\n",
    "print(y_test.shape)\n",
    "print(y_pred_reshaped.shape)\n",
    "\n",
    "y_pred_reshaped[y_pred_reshaped < 0] = 0\n",
    "\n",
    "print(y_test.mean())\n",
    "print(y_pred_reshaped.mean())\n",
    "\n",
    "y_pred_denorm = y_pred_reshaped * ref_max\n",
    "\n",
    "print(y_test.mean())\n",
    "print(y_pred_denorm.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0qjgN7s_1HCa"
   },
   "outputs": [],
   "source": [
    "n_samples = int(rows_per_day)\n",
    "plot_prediction_windows(feature, y_test, y_pred_denorm, use_active=False, n_samples=n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IzepnfpERbCQ"
   },
   "outputs": [],
   "source": [
    "n_samples = int(rows_per_day * 0.2)\n",
    "plot_prediction_windows(feature, y_test, y_pred_denorm, use_active=False, n_samples=n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DlvtG-qPRe1R"
   },
   "outputs": [],
   "source": [
    "n_samples = 80\n",
    "plot_prediction_windows(feature, y_test, y_pred_denorm, use_active=True, n_samples=n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JND48JI0TihE"
   },
   "outputs": [],
   "source": [
    "n_samples = 600\n",
    "plot_prediction_windows(feature, y_test, y_pred_denorm, use_active=True, n_samples=n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hRwlKE0cBoWM"
   },
   "outputs": [],
   "source": [
    "# train_and_present_results('ft_kettle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x7C5UZD7BsO1"
   },
   "outputs": [],
   "source": [
    "# train_and_present_results('ft_fridge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Kdqgu0kaU7S"
   },
   "outputs": [],
   "source": [
    "# train_and_present_results('ft_washing_machine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iSA4DFGzaU7S"
   },
   "outputs": [],
   "source": [
    "# train_and_present_results('ft_microwave')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mm7r340OaU7T"
   },
   "outputs": [],
   "source": [
    "# train_and_present_results('ft_dishwasher')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
